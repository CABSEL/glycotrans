{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88bb5046-8d60-45a8-86aa-0398ca13bd15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle \n",
    "import sys\n",
    "sys.path.append('pyGlyNet')\n",
    "from pyGlyNet import glycan as gly\n",
    "\n",
    "# Function to generate bin edges using ppm\n",
    "def bin_ppm(min_val, max_val, ppm):\n",
    "    value = min_val\n",
    "    bin_edges = []\n",
    "    while value < max_val:\n",
    "        bin_edges.append(value)\n",
    "        value = value * (1 + ppm/1000000)\n",
    "    \n",
    "    return bin_edges\n",
    "\n",
    "\n",
    "# Function to generate bin edges using constant bin size\n",
    "def bin_linear(min_val, max_val, size):\n",
    "    bin_edges = np.arange(min_val, max_val+size, size)\n",
    "    \n",
    "    return bin_edges\n",
    "\n",
    "\n",
    "# Function to normalize RT values\n",
    "def normalize_rt(group):\n",
    "    # Convert RT to numeric, setting non-convertible values to NaN. \n",
    "    # This is to allow RTs to be unspecified.\n",
    "    rt_numeric = pd.to_numeric(group['RT'], errors='coerce')\n",
    "    \n",
    "    # Perform normalization\n",
    "    max_rt = max(rt_numeric.max(), 30)\n",
    "    normalized_values = rt_numeric / max_rt\n",
    "    group['RT'].loc[rt_numeric.notna()] = normalized_values\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "# Function to bin RT \n",
    "def rt_binning(rt_value, RT_bin_edges):\n",
    "    if isinstance(rt_value, (int, float)):\n",
    "        # Bin the numeric value\n",
    "        binned_index = np.digitize([rt_value], RT_bin_edges)[0]\n",
    "        return f'rt{binned_index}'\n",
    "    else:\n",
    "        # Return string value, for example unspecified\n",
    "        return str(rt_value)\n",
    "    \n",
    "    \n",
    "# Function to bin MZ \n",
    "def mz_binning(mz_values, bin_edges):\n",
    "    binned_indices = np.digitize(mz_values, bin_edges)\n",
    "    return ['mz' + str(index) for index in binned_indices]\n",
    "\n",
    "\n",
    "# Function to bin peaks \n",
    "def peak_binning(peak_values, bin_edges):\n",
    "    binned_indices = np.digitize(peak_values, bin_edges)\n",
    "    return ['pk' + str(index) for index in binned_indices]\n",
    "\n",
    "\n",
    "# Function to process, normalize, sort the dictionary, and bin both keys and values\n",
    "def process_peak_d(dict_string, mz_bin_edges, peak_bin_edges, threshold):\n",
    "    try:\n",
    "        # convert peak_d to dictionary\n",
    "        dict_data = ast.literal_eval(dict_string) \n",
    "    except ValueError:\n",
    "        return [], []\n",
    "\n",
    "    # Normalize and threshold peak intensity \n",
    "    total = sum(dict_data.values())\n",
    "    normalized_dict = {k: v / total for k, v in dict_data.items() if (v / total) > threshold}\n",
    "    \n",
    "    # Sort and bin the peaks\n",
    "    sorted_dict = dict(sorted(normalized_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    mzs = list(sorted_dict.keys())\n",
    "    peaks = list(sorted_dict.values())\n",
    "    binned_mzs = mz_binning(mzs, mz_bin_edges)\n",
    "    binned_peaks = peak_binning(peaks, peak_bin_edges)\n",
    "\n",
    "    return binned_mzs, binned_peaks\n",
    "\n",
    "\n",
    "# Function to process data\n",
    "def process_data(df, RT_bin_edges, mz_bin_edges, peak_bin_edges, threshold):\n",
    "\n",
    "    # Normalize RTs from each file\n",
    "    df = df.groupby('filename', group_keys=False).apply(normalize_rt)\n",
    "\n",
    "    # Get RT bin indices\n",
    "    df['binned_RT'] = df['RT'].apply(lambda x: rt_binning(x, RT_bin_edges))\n",
    "    \n",
    "    # Get m/z bin index for precursor mass\n",
    "    df['binned_mass'] = df['reducing_mass'].apply(lambda x: 'mz' + str(np.digitize(x, mz_bin_edges)))\n",
    "    \n",
    "    # Process peak_d column\n",
    "    df['processed_peak_d'] = df['peak_d'].apply(lambda x: process_peak_d(x, mz_bin_edges, peak_bin_edges, threshold))\n",
    "\n",
    "    # Get m/z and peak bin indices\n",
    "    df['binned_mz'] = df['processed_peak_d'].apply(lambda x: x[0])\n",
    "    df['binned_peak'] = df['processed_peak_d'].apply(lambda x: x[1])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to generate corpus\n",
    "def generate_corpus_mz(df):\n",
    "\n",
    "    # Construct sentences\n",
    "    corpus = pd.DataFrame()\n",
    "    corpus['sentence'] = df[['LC_type', 'mode', 'ionization', 'modification', 'trap', 'fragmentation', 'glycan_type', 'binned_RT', 'binned_mass']].agg(' '.join, axis=1)\n",
    "    corpus['sentence'] += ' ' + df['binned_mz'].apply(' '.join)\n",
    "\n",
    "    return corpus['sentence'].tolist()\n",
    "\n",
    "def generate_corpus_mz_peak(df):\n",
    "\n",
    "    # Function to interleave mz and peak values\n",
    "    def interleave_mz_peak(mz, peak):\n",
    "        paired = [f\"{m} {p}\" for m, p in zip(mz, peak)]\n",
    "        return ' '.join(paired)\n",
    "\n",
    "    # Construct sentences\n",
    "    corpus = pd.DataFrame()\n",
    "    corpus['sentence'] = df[['LC_type', 'mode', 'ionization', 'modification', 'trap', 'fragmentation', 'glycan_type', 'binned_RT', 'binned_mass']].agg(' '.join, axis=1)\n",
    "    corpus['sentence'] += ' ' + df.apply(lambda x: interleave_mz_peak(x['binned_mz'], x['binned_peak']), axis=1)\n",
    "\n",
    "    return corpus['sentence'].tolist()\n",
    "\n",
    "# Function to generate glycan corpus\n",
    "def split_antennae(antennae):\n",
    "    # Process each line\n",
    "    processed_lines = []\n",
    "    for line in antennae:\n",
    "        # Apply replacements and stripping\n",
    "        line = line.replace('(', ' ')  # Replacing open parenthesis with space\n",
    "        line = line.replace(')', ' ')  # Replacing close parenthesis with space\n",
    "        line = line.replace(',', ' ')  # Replacing commas with space\n",
    "#        line = line.replace('{', ' ')   # Replacing open curly bracket with space\n",
    "#        line = line.replace('}', ' ')   # Replacing close curly bracket with space\n",
    "        line = line.strip()  # Removing any leading or trailing whitespace\n",
    "\n",
    "        # Add the processed line to the list\n",
    "        processed_lines.append(line)\n",
    "\n",
    "    return processed_lines\n",
    "\n",
    "def split_antenna(antenna):\n",
    "    # Apply replacements and stripping\n",
    "    antenna = antenna.replace('(', ' ')  # Replacing open parenthesis with space\n",
    "    antenna = antenna.replace(')', ' ')  # Replacing close parenthesis with space\n",
    "    antenna = antenna.replace(',', ' ')  # Replacing commas with space\n",
    "    antenna = antenna.strip()  # Removing any leading or trailing whitespace\n",
    "\n",
    "    return antenna\n",
    "\n",
    "def calculate_antenna(iupac):\n",
    "    g = gly.glycan(iupac)\n",
    "    return ' '.join(g.Antennae())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2462f28-a89d-49b7-8e86-dc89594c04db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Settings for binning\n",
    "threshold = 0.001 #peak intensity thresholding\n",
    "\n",
    "minMZ = 39.714 #minimum m/z\n",
    "maxMZ = 3000 #maximum m/z\n",
    "sizeMZ = 0.3 #m/z bin size\n",
    "mz_bin_edges = bin_linear(minMZ, maxMZ, sizeMZ) #use linear binning\n",
    "#mz_bin_edges = bin_ppm(minMZ, maxMZ, 20) #use ppm binning\n",
    "\n",
    "minI = 0\n",
    "maxI = 1\n",
    "sizeI = 0.001 #peak intensity bin size\n",
    "peak_bin_edges = bin_linear(minI, maxI, sizeI)\n",
    "\n",
    "minRT = 0\n",
    "maxRT = 1\n",
    "sizeRT = 0.01 #relative retention time bin size\n",
    "RT_bin_edges = bin_linear(minRT, maxRT, sizeRT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f861f3c-026d-4fcb-8b65-b609eeae720c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "dataDir = '/Users/rudi/Data/CandyCrunch/Apr2024/'\n",
    "fulldata_file = ''.join([dataDir, 'training.xlsx'])\n",
    "\n",
    "# Load full dataset\n",
    "df = pd.read_excel(fulldata_file)\n",
    "\n",
    "# Process data\n",
    "df = process_data(df, RT_bin_edges, mz_bin_edges, peak_bin_edges, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3eaf7d4-83c7-46d8-a6e8-78af0af82920",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace glycan labels with antennae\n",
    "# Get the value counts of unique glycans and reset the index to turn it into a DataFrame\n",
    "glycan_labels = df['glycan'].value_counts().reset_index()\n",
    "glycan_labels.columns = ['glycan', 'frequency']\n",
    "glycan_labels = glycan_labels.sort_values(by='glycan').reset_index(drop=True)\n",
    "\n",
    "# Create a mapping from glycans to their labels (indices)\n",
    "glycan_to_antenna = {row['glycan']: calculate_antenna(row['glycan']) for _, row in glycan_labels.iterrows()}\n",
    "\n",
    "# Search glycan_to_label for glycans with curly brackets and mark them for removal\n",
    "for glycan in glycan_to_antenna.keys():\n",
    "    if '{' in glycan or '}' in glycan:\n",
    "        glycan_to_antenna[glycan] = 'remove'\n",
    "\n",
    "# Assign class label to spectra\n",
    "df['antenna'] = df['glycan'].map(glycan_to_antenna)\n",
    "\n",
    "# Remove rows where class_label is 'remove'\n",
    "df = df[df['antenna'] != 'remove']\n",
    "\n",
    "# Split antenna\n",
    "df['antenna'] = df['antenna'].apply(split_antenna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e69d60e7-9fa2-450a-9eef-8e7781fd1342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare vocab for MS\n",
    "# Initialize an empty set to store unique words\n",
    "words = set()\n",
    "\n",
    "# Loop through each specified column\n",
    "for column in ['LC_type', 'mode', 'ionization', 'modification', 'trap', 'fragmentation', 'glycan_type']:\n",
    "    # Extract unique values from the column and put them into words\n",
    "    words.update(df[column].astype(str).dropna().unique())\n",
    "\n",
    "# Convert the set to a list\n",
    "words = list(words)\n",
    "\n",
    "# Prepare vocabulary for m/z\n",
    "for index in range(0, len(mz_bin_edges) + 1):\n",
    "    words.append('mz' + str(index))\n",
    "\n",
    "# Prepare vocabulary for retention time\n",
    "for index in range(0, len(RT_bin_edges) + 1):\n",
    "    words.append('rt' + str(index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7496b5b1-aa1b-4610-b008-315e06a88ce9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GalOMe\n",
      "ManOS\n",
      "Gal\n",
      "Glc\n",
      "HexNAc\n",
      "GalNAcOPCho\n",
      "Fuc\n",
      "GalNAcOMe\n",
      "GlcOS\n",
      "IdoA2S\n",
      "GlcNS6S\n",
      ".1-3\n",
      ".1-4\n",
      "IdoA\n",
      "HexA\n",
      "GalNAc\n",
      "GlcNAc6S\n",
      "GalNAc4S\n",
      "Gal6S\n",
      "Ins\n",
      "a2-.\n",
      ".1-6\n",
      "GalN\n",
      "GlcOP\n",
      "b1-3\n",
      "GalNAc6S\n",
      "GalNAcOS\n",
      "Man\n",
      "Glc-ol\n",
      "Rha\n",
      "ManOMe\n",
      "GlcNAc\n",
      "FucOS\n",
      "1-.\n",
      "a1-.\n",
      "Ara\n",
      "GlcN\n",
      "GlcN6S\n",
      "Xyl\n",
      "Man6P\n",
      "GlcNAcOS\n",
      "HexNAcOS\n",
      "b1-.\n",
      "a1-4\n",
      "GalOS\n",
      "GlcA\n",
      "Neu5Ac\n",
      "HexA2S\n",
      "Gal4S\n",
      "GlcNS3S6S\n",
      "a1-6\n",
      "a2-8\n",
      "a2-6\n",
      "a1-3\n",
      "Neu5Gc\n",
      "a2-3\n",
      "Rha3S\n",
      "Hex\n",
      "GlcNS3S\n",
      "b1-4\n",
      "b1-6\n",
      ".1-.\n",
      "b1-2\n",
      "Gal3S\n",
      "Neu5Ac8S\n",
      "Kdn\n",
      "a1-2\n",
      "GlcNS\n"
     ]
    }
   ],
   "source": [
    "# Prepare glycan vocab \n",
    "antennae = df['antenna'].unique().tolist()\n",
    "antennae = split_antennae(antennae)\n",
    "\n",
    "# If desired, print out the processed antennae:\n",
    "#for line in antennae:\n",
    "#    print(line)\n",
    "\n",
    "# Get unique glycan words\n",
    "glycan_words = []\n",
    "for line in antennae:\n",
    "    # First split by space\n",
    "    glycan_words = glycan_words + line.split()\n",
    "\n",
    "# Filter out any empty strings that might have been generated\n",
    "glycan_words = [word for word in glycan_words if word]\n",
    "glycan_vocab = list(set(glycan_words))  \n",
    "\n",
    "# To check the list of unique words:\n",
    "for word in glycan_vocab:\n",
    "    print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28da4b70-8cbc-4d3d-a9b8-56b5644d40a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary file created with 10073 words.\n"
     ]
    }
   ],
   "source": [
    "# Prepare vocabulary\n",
    "\n",
    "words = words + glycan_vocab\n",
    "\n",
    "# Write the unique words to a file\n",
    "with open('vocab.txt', 'w') as file:\n",
    "    for word in words:\n",
    "        file.write(word + '\\n')\n",
    "\n",
    "print(f\"Vocabulary file created with {len(words)} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cf647eb-8a22-4aef-813e-b080414db45d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train set:  426688\n",
      "Size of test set:  75299\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test\n",
    "train_df, test_df = train_test_split(df, test_size=0.15, random_state=0)\n",
    "\n",
    "print('Size of train set: ', len(train_df))\n",
    "print('Size of test set: ', len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f655a975-f8d5-48a6-8ee3-01ebaef1e492",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create text corpus for spectra using mz only\n",
    "sentences = generate_corpus_mz(train_df)\n",
    "\n",
    "with open('train_corpus_mz_bart.txt', 'w') as file:\n",
    "    for sentence in sentences:\n",
    "        file.write(sentence + '\\n')\n",
    "\n",
    "sentences = generate_corpus_mz(test_df)\n",
    "\n",
    "with open('test_corpus_mz_bart.txt', 'w') as file:\n",
    "    for sentence in sentences:\n",
    "        file.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8157ce62-3f1c-4b48-bdfa-0a3841e18f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create text corpus for glycan antennae\n",
    "\n",
    "with open('train_corpus_glycan.txt', 'w') as file:\n",
    "    for antenna in train_df['antenna']:\n",
    "        file.write(antenna + '\\n')\n",
    "        \n",
    "with open('test_corpus_glycan.txt', 'w') as file:\n",
    "    for antenna in test_df['antenna']:\n",
    "        file.write(antenna + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f755e335-aa0d-4e5c-b900-d6c145c42702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text corpus for spectra for full training\n",
    "sentences = generate_corpus_mz(df)\n",
    "\n",
    "with open('corpus_mz_bart.txt', 'w') as file:\n",
    "    for sentence in sentences:\n",
    "        file.write(sentence + '\\n')\n",
    "\n",
    "with open('corpus_glycan.txt', 'w') as file:\n",
    "    for antenna in df['antenna']:\n",
    "        file.write(antenna + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
