{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b4fbd55-33de-432d-955b-bf8e6a6df006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle \n",
    "\n",
    "\n",
    "# Function to generate bin edges using constant bin size\n",
    "def bin_linear(min_val, max_val, size):\n",
    "    bin_edges = np.arange(min_val, max_val+size, size)\n",
    "    \n",
    "    return bin_edges\n",
    "\n",
    "\n",
    "# Function to normalize RT values\n",
    "def normalize_rt(group):\n",
    "    # Convert RT to numeric, setting non-convertible values to NaN. \n",
    "    # This is to allow RTs to be unspecified.\n",
    "    rt_numeric = pd.to_numeric(group['RT'], errors='coerce')\n",
    "    \n",
    "    # Perform normalization\n",
    "    max_rt = max(rt_numeric.max(), 30)\n",
    "    normalized_values = rt_numeric / max_rt\n",
    "    group['RT'].loc[rt_numeric.notna()] = normalized_values\n",
    "\n",
    "    return group\n",
    "\n",
    "\n",
    "# Function to bin RT \n",
    "def rt_binning(rt_value, RT_bin_edges):\n",
    "    if isinstance(rt_value, (int, float)):\n",
    "        # Bin the numeric value\n",
    "        binned_index = np.digitize([rt_value], RT_bin_edges)[0]\n",
    "        return f'rt{binned_index}'\n",
    "    else:\n",
    "        # Return string value, for example unspecified\n",
    "        return str(rt_value)\n",
    "    \n",
    "    \n",
    "# Function to bin MZ \n",
    "def mz_binning(mz_values, bin_edges):\n",
    "    binned_indices = np.digitize(mz_values, bin_edges)\n",
    "    return ['mz' + str(index) for index in binned_indices]\n",
    "\n",
    "\n",
    "# Function to bin peaks \n",
    "def peak_binning(peak_values, bin_edges):\n",
    "    binned_indices = np.digitize(peak_values, bin_edges)\n",
    "    return ['pk' + str(index) for index in binned_indices]\n",
    "\n",
    "\n",
    "# Function to process, normalize, sort the dictionary, and bin both keys and values\n",
    "def process_peak_d(dict_string, mz_bin_edges, peak_bin_edges, threshold):\n",
    "    try:\n",
    "        # convert peak_d to dictionary\n",
    "        dict_data = ast.literal_eval(dict_string) \n",
    "    except ValueError:\n",
    "        return [], []\n",
    "\n",
    "    # Normalize and threshold peak intensity \n",
    "    total = sum(dict_data.values())\n",
    "    normalized_dict = {k: v / total for k, v in dict_data.items() if (v / total) > threshold}\n",
    "    \n",
    "    # Sort and bin the peaks\n",
    "    sorted_dict = dict(sorted(normalized_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    mzs = list(sorted_dict.keys())\n",
    "    peaks = list(sorted_dict.values())\n",
    "    binned_mzs = mz_binning(mzs, mz_bin_edges)\n",
    "    binned_peaks = peak_binning(peaks, peak_bin_edges)\n",
    "\n",
    "    return binned_mzs, binned_peaks\n",
    "\n",
    "\n",
    "# Function to process data\n",
    "def process_data(df, RT_bin_edges, mz_bin_edges, peak_bin_edges, threshold):\n",
    "\n",
    "    # Normalize RTs from each file\n",
    "    df = df.groupby('filename', group_keys=False).apply(normalize_rt)\n",
    "\n",
    "    # Get RT bin indices\n",
    "    df['binned_RT'] = df['RT'].apply(lambda x: rt_binning(x, RT_bin_edges))\n",
    "    \n",
    "    # Get m/z bin index for precursor mass\n",
    "    df['binned_mass'] = df['reducing_mass'].apply(lambda x: 'mz' + str(np.digitize(x, mz_bin_edges)))\n",
    "    \n",
    "    # Process peak_d column\n",
    "    df['processed_peak_d'] = df['peak_d'].apply(lambda x: process_peak_d(x, mz_bin_edges, peak_bin_edges, threshold))\n",
    "\n",
    "    # Get m/z and peak bin indices\n",
    "    df['binned_mz'] = df['processed_peak_d'].apply(lambda x: x[0])\n",
    "    df['binned_peak'] = df['processed_peak_d'].apply(lambda x: x[1])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to generate corpus\n",
    "def generate_corpus_mz(df):\n",
    "\n",
    "    # Construct sentences\n",
    "    corpus = pd.DataFrame()\n",
    "    corpus['sentence'] = df[['LC_type', 'mode', 'ionization', 'modification', 'trap', 'fragmentation', 'glycan_type', 'binned_RT', 'binned_mass']].agg(' '.join, axis=1)\n",
    "    corpus['sentence'] += ' ' + df['binned_mz'].apply(' '.join)\n",
    "\n",
    "    return corpus['sentence'].tolist()\n",
    "\n",
    "def generate_corpus_mz_peak(df):\n",
    "\n",
    "    # Function to interleave mz and peak values\n",
    "    def interleave_mz_peak(mz, peak):\n",
    "        paired = [f\"{m} {p}\" for m, p in zip(mz, peak)]\n",
    "        return ' '.join(paired)\n",
    "\n",
    "    # Construct sentences\n",
    "    corpus = pd.DataFrame()\n",
    "    corpus['sentence'] = df[['LC_type', 'mode', 'ionization', 'modification', 'trap', 'fragmentation', 'glycan_type', 'binned_RT', 'binned_mass']].agg(' '.join, axis=1)\n",
    "    corpus['sentence'] += ' ' + df.apply(lambda x: interleave_mz_peak(x['binned_mz'], x['binned_peak']), axis=1)\n",
    "\n",
    "    return corpus['sentence'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f861f3c-026d-4fcb-8b65-b609eeae720c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "# MSMS dataset can be downloaded from Zenodo at https://doi.org/10.5281/zenodo.15741423\n",
    "# Please specify the path to the directory in which the dataset.xlsx is saved\n",
    "dataDir = '/path/to/dataset/directory/'  \n",
    "fulldata_file = ''.join([dataDir, 'dataset.xlsx'])\n",
    "\n",
    "# Load full dataset\n",
    "df_raw = pd.read_excel(fulldata_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22edb82a-db85-4ac6-b4fc-7d6ac476e70e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bins:  9869\n"
     ]
    }
   ],
   "source": [
    "# Settings for binning\n",
    "# Do not change the settings\n",
    "threshold = 0.001 #peak intensity thresholding\n",
    "\n",
    "minMZ = 39.714 #minimum m/z\n",
    "maxMZ = 3000 #maximum m/z\n",
    "sizeMZ = 0.3 #m/z bin size\n",
    "mz_bin_edges = bin_linear(minMZ, maxMZ, sizeMZ)\n",
    "\n",
    "minI = 0\n",
    "maxI = 1\n",
    "sizeI = 0.001 #peak intensity bin size\n",
    "peak_bin_edges = bin_linear(minI, maxI, sizeI)\n",
    "\n",
    "minRT = 0\n",
    "maxRT = 1\n",
    "sizeRT = 0.01 #relative retention time bin size\n",
    "RT_bin_edges = bin_linear(minRT, maxRT, sizeRT)\n",
    "\n",
    "print('Number of bins: ', len(mz_bin_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8be498d9-839d-445a-a809-4c3000cb36f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process data\n",
    "df = process_data(df_raw, RT_bin_edges, mz_bin_edges, peak_bin_edges, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc97b22a-deb5-42d7-a5f9-c87b450cd86b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary file created with 10005 words.\n"
     ]
    }
   ],
   "source": [
    "# Prepare vocabulary\n",
    "# Initialize an empty set to store unique words\n",
    "words = set()\n",
    "\n",
    "# Loop through each specified column\n",
    "for column in ['LC_type', 'mode', 'ionization', 'modification', 'trap', 'fragmentation', 'glycan_type']:\n",
    "    # Extract unique values from the column and put them into words\n",
    "    words.update(df[column].astype(str).dropna().unique())\n",
    "\n",
    "# Convert the set to a list\n",
    "words = list(words)\n",
    "\n",
    "# Prepare vocabulary for m/z\n",
    "for index in range(0, len(mz_bin_edges) + 1):\n",
    "    words.append('mz' + str(index))\n",
    "    \n",
    "# Prepare vocabulary for retention time\n",
    "for index in range(0, len(RT_bin_edges) + 1):\n",
    "    words.append('rt' + str(index))\n",
    "    \n",
    "# Write the unique words to a file\n",
    "with open('vocab_glycobert.txt', 'w') as file:      # Change the path to save the vocab_glycobert.txt file if needed\n",
    "    for word in sorted(words):\n",
    "        file.write(word + '\\n')\n",
    "\n",
    "print(f\"Vocabulary file created with {len(words)} words.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da1e753c-abea-4869-b06e-b431fc07cd54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of glycan classes:  3590\n"
     ]
    }
   ],
   "source": [
    "# Prepare glycan class labels\n",
    "\n",
    "# Get the value counts of unique glycans\n",
    "glycan_labels = df['glycan'].value_counts()\n",
    "\n",
    "# Reset the index to turn it into a DataFrame\n",
    "glycan_labels = glycan_labels.reset_index()\n",
    "glycan_labels.columns = ['glycan', 'frequency']\n",
    "glycan_labels = glycan_labels.sort_values(by='glycan')\n",
    "glycan_labels = glycan_labels.reset_index()\n",
    "\n",
    "# Create a mapping from glycans to their labels (indices)\n",
    "glycan_to_label = {row['glycan']: idx for idx, row in glycan_labels.iterrows()}\n",
    "\n",
    "# Map each glycan in df to its corresponding label\n",
    "df['class_label'] = df['glycan'].map(glycan_to_label)\n",
    "\n",
    "# Display the first few rows of the DataFrame with class labels\n",
    "print('Number of glycan classes: ', len(glycan_labels))\n",
    "\n",
    "glycan_labels_df = pd.DataFrame(list(glycan_to_label.items()), columns=['glycan', 'label'])\n",
    "\n",
    "# Define the file path for the Excel file\n",
    "excel_out = ''.join([dataDir, 'glycans.xlsx'])  # Replace with your desired file path\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "glycan_labels_df.to_excel(excel_out, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2683cf23-2529-4e23-8358-b6483d0b161d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train set:  428074\n",
      "Size of test set:  75372\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test\n",
    "train_df, test_df = train_test_split(df, test_size=0.15, random_state=0)\n",
    "\n",
    "# Find class_labels present in test but not in train\n",
    "labels_train = set(train_df['class_label'])\n",
    "labels_test = set(test_df['class_label'])\n",
    "labels_move = labels_test - labels_train\n",
    "\n",
    "# Move rows with these labels from test to train\n",
    "rows_to_move = test_df[test_df['class_label'].isin(labels_move)]\n",
    "train_df = pd.concat([train_df, rows_to_move])\n",
    "test_df = test_df[~test_df['class_label'].isin(labels_move)]\n",
    "\n",
    "# Now, train_df and test_df are your final DataFrames\n",
    "print('Size of train set: ', len(train_df))\n",
    "print('Size of test set: ', len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdca1fdf-131b-40bd-9d7d-4e477e6061de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sentences saved to train_corpus_mz.txt\n",
      "Test sentences saved to test_corpus_mz.txt\n"
     ]
    }
   ],
   "source": [
    "# Specify path to save train and test mz corpus files\n",
    "train_mz_corpus_path = 'train_corpus_mz.txt'     # Change the path to save the train mz corpus if needed\n",
    "test_mz_corpus_path  = 'test_corpus_mz.txt'      # Change the path to save the test mz corpus if needed\n",
    "\n",
    "# Generate corpus for model training\n",
    "sentences = generate_corpus_mz(train_df)\n",
    "\n",
    "with open(train_mz_corpus_path, 'w') as file:  \n",
    "    for sentence in sentences:\n",
    "        file.write(sentence + '\\n')\n",
    "\n",
    "print(f\"Train sentences saved to {train_mz_corpus_path}\")\n",
    "\n",
    "sentences = generate_corpus_mz(test_df)\n",
    "\n",
    "with open(test_mz_corpus_path, 'w') as file:    \n",
    "    for sentence in sentences:\n",
    "        file.write(sentence + '\\n')\n",
    "\n",
    "print(f\"Test sentences saved to {test_mz_corpus_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bf755d8-466d-4e00-a718-7401f3e48667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save train and test glycan class label\n",
    "\n",
    "train_labels = train_df['class_label'].tolist()\n",
    "test_labels = test_df['class_label'].tolist()\n",
    "\n",
    "train_label_path = 'train_label.pkl'    # Change the path to save the train labels pickle file if needed\n",
    "test_label_path = 'test_label.pkl'      # Change the path to save the test labels pickle file if needed\n",
    "\n",
    "# Save to pickle files\n",
    "with open(train_label_path, 'wb') as file:\n",
    "    pickle.dump(train_labels, file)\n",
    "\n",
    "# Save test_labels list to a pickle file\n",
    "with open(test_label_path, 'wb') as file:\n",
    "    pickle.dump(test_labels, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ed02745-4a6d-4779-ae46-3ebe985efc30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final set of sentences saved to corpus_mz.txt\n"
     ]
    }
   ],
   "source": [
    "# Specify path to full mz corpus file\n",
    "full_mz_corpus_path = 'full_corpus_mz.txt'   # Change the path to save the full mz corpus if needed\n",
    "\n",
    "# Generate corpus for model training using full dataset\n",
    "sentences = generate_corpus_mz(df)\n",
    "\n",
    "with open(full_mz_corpus_path, 'w') as file:\n",
    "    for sentence in sentences:\n",
    "        file.write(sentence + '\\n')\n",
    "\n",
    "print(f\"Final set of sentences saved to {full_mz_corpus_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b52e936-81a4-4db0-93a7-c1027f41db23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load glycan labels\n",
    "glycans_file = ''.join([dataDir, 'glycans.xlsx'])\n",
    "glycans_df = pd.read_excel(glycans_file)\n",
    "\n",
    "# Create a mapping from glycans to their labels (indices)\n",
    "glycan_to_label = {row['glycan']: row['label'] for i, row in glycans_df.iterrows()}\n",
    "\n",
    "# Map each glycan in df to its corresponding label\n",
    "df['class_label'] = df['glycan'].map(glycan_to_label)\n",
    "labels = df['class_label'].tolist()\n",
    "\n",
    "# Save labels list to a pickle file\n",
    "label_file = 'label.pkl'                     # Change the path to save the glycan label file if needed\n",
    "with open(label_file, 'wb') as file:\n",
    "    pickle.dump(labels, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
